{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tools & Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Get the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>2053</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>2</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>1936</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>6227</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid   wid reflection_period  \\\n",
       "0  27673  2053               24h   \n",
       "1  27674     2               24h   \n",
       "2  27675  1936               24h   \n",
       "3  27676   206               24h   \n",
       "4  27677  6227               24h   \n",
       "\n",
       "                                         original_hm  \\\n",
       "0  I went on a successful date with someone I fel...   \n",
       "1  I was happy when my son got 90% marks in his e...   \n",
       "2       I went to the gym this morning and did yoga.   \n",
       "3  We had a serious talk with some friends of our...   \n",
       "4  I went with grandchildren to butterfly display...   \n",
       "\n",
       "                                          cleaned_hm  modified  num_sentence  \\\n",
       "0  I went on a successful date with someone I fel...      True             1   \n",
       "1  I was happy when my son got 90% marks in his e...      True             1   \n",
       "2       I went to the gym this morning and did yoga.      True             1   \n",
       "3  We had a serious talk with some friends of our...      True             2   \n",
       "4  I went with grandchildren to butterfly display...      True             1   \n",
       "\n",
       "  ground_truth_category predicted_category  \n",
       "0                   NaN          affection  \n",
       "1                   NaN          affection  \n",
       "2                   NaN           exercise  \n",
       "3               bonding            bonding  \n",
       "4                   NaN          affection  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cleaned_hm.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Basic data cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affection           34168\n",
       "achievement         33993\n",
       "enjoy_the_moment    11144\n",
       "bonding             10727\n",
       "leisure              7458\n",
       "nature               1843\n",
       "exercise             1202\n",
       "Name: predicted_category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"predicted_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     83711\n",
       "2      9542\n",
       "3      3847\n",
       "4      1624\n",
       "5       821\n",
       "6       336\n",
       "7       183\n",
       "8       107\n",
       "10       68\n",
       "9        61\n",
       "11       35\n",
       "13       26\n",
       "12       21\n",
       "16       17\n",
       "18       17\n",
       "14       14\n",
       "17       14\n",
       "19       12\n",
       "21       10\n",
       "25        7\n",
       "15        7\n",
       "23        7\n",
       "24        5\n",
       "26        5\n",
       "22        4\n",
       "29        3\n",
       "31        3\n",
       "30        3\n",
       "20        3\n",
       "27        2\n",
       "32        2\n",
       "37        2\n",
       "40        2\n",
       "56        1\n",
       "46        1\n",
       "53        1\n",
       "51        1\n",
       "48        1\n",
       "69        1\n",
       "35        1\n",
       "45        1\n",
       "44        1\n",
       "42        1\n",
       "58        1\n",
       "34        1\n",
       "28        1\n",
       "60        1\n",
       "Name: num_sentence, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"num_sentence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affection           34020\n",
       "achievement         33966\n",
       "enjoy_the_moment    11115\n",
       "bonding             10700\n",
       "leisure              7458\n",
       "nature               1839\n",
       "exercise             1202\n",
       "Name: predicted_category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting happy moments with more than 10 sentences\n",
    "mod_data = data.loc[data['num_sentence'] <= 10]\n",
    "mod_data[\"predicted_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categorical to numerical\n",
    "encode = {\n",
    "    \"affection\" : 0,\n",
    "    \"achievement\"  : 1,       \n",
    "    \"bonding\" : 2,    \n",
    "    \"enjoy_the_moment\" : 3,     \n",
    "    \"leisure\"  : 4,    \n",
    "    \"nature\" : 5,    \n",
    "    \"exercise\" : 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>2053</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>2</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>1936</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>6227</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid   wid reflection_period  \\\n",
       "0  27673  2053               24h   \n",
       "1  27674     2               24h   \n",
       "2  27675  1936               24h   \n",
       "3  27676   206               24h   \n",
       "4  27677  6227               24h   \n",
       "\n",
       "                                         original_hm  \\\n",
       "0  I went on a successful date with someone I fel...   \n",
       "1  I was happy when my son got 90% marks in his e...   \n",
       "2       I went to the gym this morning and did yoga.   \n",
       "3  We had a serious talk with some friends of our...   \n",
       "4  I went with grandchildren to butterfly display...   \n",
       "\n",
       "                                          cleaned_hm  modified  num_sentence  \\\n",
       "0  I went on a successful date with someone I fel...      True             1   \n",
       "1  I was happy when my son got 90% marks in his e...      True             1   \n",
       "2       I went to the gym this morning and did yoga.      True             1   \n",
       "3  We had a serious talk with some friends of our...      True             2   \n",
       "4  I went with grandchildren to butterfly display...      True             1   \n",
       "\n",
       "  ground_truth_category  predicted_category  \n",
       "0                   NaN                   0  \n",
       "1                   NaN                   0  \n",
       "2                   NaN                   6  \n",
       "3               bonding                   2  \n",
       "4                   NaN                   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data[\"predicted_category\"] = mod_data[\"predicted_category\"].apply(lambda x: encode[x])\n",
    "mod_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data cleaning for NLP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'went',\n",
       "  'on',\n",
       "  'a',\n",
       "  'successful',\n",
       "  'date',\n",
       "  'with',\n",
       "  'someone',\n",
       "  'i',\n",
       "  'felt',\n",
       "  'sympathy',\n",
       "  'and',\n",
       "  'connection',\n",
       "  'with'],\n",
       " ['i',\n",
       "  'was',\n",
       "  'happy',\n",
       "  'when',\n",
       "  'my',\n",
       "  'son',\n",
       "  'got',\n",
       "  'marks',\n",
       "  'in',\n",
       "  'his',\n",
       "  'examination'],\n",
       " ['i', 'went', 'to', 'the', 'gym', 'this', 'morning', 'and', 'did', 'yoga'],\n",
       " ['we',\n",
       "  'had',\n",
       "  'a',\n",
       "  'serious',\n",
       "  'talk',\n",
       "  'with',\n",
       "  'some',\n",
       "  'friends',\n",
       "  'of',\n",
       "  'ours',\n",
       "  'who',\n",
       "  'have',\n",
       "  'been',\n",
       "  'flaky',\n",
       "  'lately',\n",
       "  'they',\n",
       "  'understood',\n",
       "  'and',\n",
       "  'we',\n",
       "  'had',\n",
       "  'a',\n",
       "  'good',\n",
       "  'evening',\n",
       "  'hanging',\n",
       "  'out'],\n",
       " ['i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'grandchildren',\n",
       "  'to',\n",
       "  'butterfly',\n",
       "  'display',\n",
       "  'at',\n",
       "  'crohn',\n",
       "  'conservatory'],\n",
       " ['i', 'meditated', 'last', 'night'],\n",
       " ['i',\n",
       "  'made',\n",
       "  'a',\n",
       "  'new',\n",
       "  'recipe',\n",
       "  'for',\n",
       "  'peasant',\n",
       "  'bread',\n",
       "  'and',\n",
       "  'it',\n",
       "  'came',\n",
       "  'out',\n",
       "  'spectacular'],\n",
       " ['i',\n",
       "  'got',\n",
       "  'gift',\n",
       "  'from',\n",
       "  'my',\n",
       "  'elder',\n",
       "  'brother',\n",
       "  'which',\n",
       "  'was',\n",
       "  'really',\n",
       "  'surprising',\n",
       "  'me'],\n",
       " ['yesterday', 'my', 'moms', 'birthday', 'so', 'i', 'enjoyed'],\n",
       " ['watching', 'cupcake', 'wars', 'with', 'my', 'three', 'teen', 'children'],\n",
       " ['i',\n",
       "  'came',\n",
       "  'in',\n",
       "  'place',\n",
       "  'in',\n",
       "  'my',\n",
       "  'call',\n",
       "  'of',\n",
       "  'duty',\n",
       "  'video',\n",
       "  'game'],\n",
       " ['i',\n",
       "  'completed',\n",
       "  'my',\n",
       "  'miles',\n",
       "  'run',\n",
       "  'without',\n",
       "  'break',\n",
       "  'it',\n",
       "  'makes',\n",
       "  'me',\n",
       "  'feel',\n",
       "  'strong'],\n",
       " ['went', 'to', 'movies', 'with', 'my', 'friends', 'it', 'was', 'fun'],\n",
       " ['i', 'was', 'shorting', 'gold', 'and', 'made', 'from', 'the', 'trade'],\n",
       " ['hearing',\n",
       "  'songs',\n",
       "  'it',\n",
       "  'can',\n",
       "  'be',\n",
       "  'nearly',\n",
       "  'impossible',\n",
       "  'to',\n",
       "  'go',\n",
       "  'from',\n",
       "  'angry',\n",
       "  'to',\n",
       "  'happy',\n",
       "  'so',\n",
       "  'you',\n",
       "  're',\n",
       "  'just',\n",
       "  'looking',\n",
       "  'for',\n",
       "  'the',\n",
       "  'thought',\n",
       "  'that',\n",
       "  'eases',\n",
       "  'you',\n",
       "  'out',\n",
       "  'of',\n",
       "  'your',\n",
       "  'angry',\n",
       "  'feeling',\n",
       "  'and',\n",
       "  'moves',\n",
       "  'you',\n",
       "  'in',\n",
       "  'the',\n",
       "  'direction',\n",
       "  'of',\n",
       "  'happiness',\n",
       "  'it',\n",
       "  'may',\n",
       "  'take',\n",
       "  'a',\n",
       "  'while',\n",
       "  'but',\n",
       "  'as',\n",
       "  'long',\n",
       "  'as',\n",
       "  'you',\n",
       "  're',\n",
       "  'headed',\n",
       "  'in',\n",
       "  'a',\n",
       "  'more',\n",
       "  'positive',\n",
       "  'direction',\n",
       "  'youall',\n",
       "  'be',\n",
       "  'doing',\n",
       "  'yourself',\n",
       "  'a',\n",
       "  'world',\n",
       "  'of',\n",
       "  'good'],\n",
       " ['my', 'son', 'performed', 'very', 'well', 'for', 'a', 'test', 'preparation'],\n",
       " ['i', 'helped', 'my', 'neighbour', 'to', 'fix', 'their', 'car', 'damages'],\n",
       " ['managed',\n",
       "  'to',\n",
       "  'get',\n",
       "  'the',\n",
       "  'final',\n",
       "  'trophy',\n",
       "  'in',\n",
       "  'a',\n",
       "  'game',\n",
       "  'i',\n",
       "  'was',\n",
       "  'playing'],\n",
       " ['a',\n",
       "  'hot',\n",
       "  'kiss',\n",
       "  'with',\n",
       "  'my',\n",
       "  'girl',\n",
       "  'friend',\n",
       "  'last',\n",
       "  'night',\n",
       "  'made',\n",
       "  'my',\n",
       "  'day'],\n",
       " ['my',\n",
       "  'new',\n",
       "  'bcaas',\n",
       "  'came',\n",
       "  'in',\n",
       "  'the',\n",
       "  'mail',\n",
       "  'yay',\n",
       "  'strawberry',\n",
       "  'lemonade',\n",
       "  'flavored',\n",
       "  'aminos',\n",
       "  'make',\n",
       "  'my',\n",
       "  'heart',\n",
       "  'happy']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "happy_lines = list()\n",
    "lines = mod_data[\"cleaned_hm\"].values.tolist()\n",
    "\n",
    "for line in lines:\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove puntuations\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove non alphabetic characters\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    happy_lines.append(words)\n",
    "    \n",
    "happy_lines[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training Word2Vec model on HappyDB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26183\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = gensim.models.Word2Vec(sentences=happy_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "vocab_words = list(model.wv.vocab)\n",
    "print(len(vocab_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Saving the word vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"happydb_word2vec.txt\"\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading the word vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'happydb_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Train-Test split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26183 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "max_length = 55\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(happy_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(happy_lines)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "sentiment =  mod_data['predicted_category'].values\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "n_values = np.max(sentiment) + 1\n",
    "Y = np.eye(n_values)[sentiment]\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = Y[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = Y[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (80240, 55)\n",
      "Shape of y_train tensor: (80240, 7)\n",
      "Shape of X_test_pad tensor: (20060, 55)\n",
      "Shape of y_test tensor: (20060, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Making the embedding matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26184\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *GRU model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 55, 100)           2618400   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                31680     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 2,650,535\n",
      "Trainable params: 32,135\n",
      "Non-trainable params: 2,618,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=64,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Save the best model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', verbose=1, monitor='val_acc',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80240 samples, validate on 20060 samples\n",
      "Epoch 1/15\n",
      " - 67s - loss: 1.3175 - acc: 0.4845 - val_loss: 0.6625 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77144, saving model to model-001-0.484459-0.771436.h5\n",
      "Epoch 2/15\n",
      " - 56s - loss: 0.5469 - acc: 0.8115 - val_loss: 0.4394 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77144 to 0.84307, saving model to model-002-0.811503-0.843071.h5\n",
      "Epoch 3/15\n",
      " - 58s - loss: 0.4476 - acc: 0.8397 - val_loss: 0.3945 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84307 to 0.85628, saving model to model-003-0.839718-0.856281.h5\n",
      "Epoch 4/15\n",
      " - 56s - loss: 0.4141 - acc: 0.8504 - val_loss: 0.3766 - val_acc: 0.8626\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85628 to 0.86256, saving model to model-004-0.850436-0.862562.h5\n",
      "Epoch 5/15\n",
      " - 55s - loss: 0.3924 - acc: 0.8571 - val_loss: 0.3728 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86256 to 0.86351, saving model to model-005-0.857091-0.863509.h5\n",
      "Epoch 6/15\n",
      " - 55s - loss: 0.3814 - acc: 0.8618 - val_loss: 0.3587 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86351 to 0.86979, saving model to model-006-0.861765-0.869791.h5\n",
      "Epoch 7/15\n",
      " - 56s - loss: 0.3704 - acc: 0.8655 - val_loss: 0.3513 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.86979 to 0.87069, saving model to model-007-0.865454-0.870688.h5\n",
      "Epoch 8/15\n",
      " - 57s - loss: 0.3662 - acc: 0.8660 - val_loss: 0.3453 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.87069 to 0.87278, saving model to model-008-0.865977-0.872782.h5\n",
      "Epoch 9/15\n",
      " - 57s - loss: 0.3577 - acc: 0.8678 - val_loss: 0.3443 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.87278 to 0.87338, saving model to model-009-0.867772-0.873380.h5\n",
      "Epoch 10/15\n",
      " - 56s - loss: 0.3542 - acc: 0.8697 - val_loss: 0.3437 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.87338 to 0.87438, saving model to model-010-0.869716-0.874377.h5\n",
      "Epoch 11/15\n",
      " - 56s - loss: 0.3477 - acc: 0.8717 - val_loss: 0.3356 - val_acc: 0.8753\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.87438 to 0.87527, saving model to model-011-0.871710-0.875274.h5\n",
      "Epoch 12/15\n",
      " - 54s - loss: 0.3429 - acc: 0.8729 - val_loss: 0.3348 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.87527 to 0.87642, saving model to model-012-0.872944-0.876421.h5\n",
      "Epoch 13/15\n",
      " - 57s - loss: 0.3408 - acc: 0.8746 - val_loss: 0.3321 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.87642 to 0.87687, saving model to model-013-0.874576-0.876869.h5\n",
      "Epoch 14/15\n",
      " - 57s - loss: 0.3358 - acc: 0.8752 - val_loss: 0.3340 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87687\n",
      "Epoch 15/15\n",
      " - 57s - loss: 0.3323 - acc: 0.8772 - val_loss: 0.3266 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.87687 to 0.87901, saving model to model-015-0.877193-0.879013.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27556bd3710>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=15, validation_data=(X_test_pad, y_test), callbacks=[checkpoint], verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
